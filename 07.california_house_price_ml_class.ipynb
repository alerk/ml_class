{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first task is to build a model of housing prices in California using the California census data. This data has metrics such as the population, median income, median housing price, and so on for each block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people)."},{"metadata":{},"cell_type":"markdown","source":"A sequence of data processing components is called a data pipeline. Pipelines are very common in Machine Learning systems, since there is a lot of data to manipulate and many data transformations to apply."},{"metadata":{},"cell_type":"markdown","source":"Frame the problem: is it supervised, unsupervised, or Reinforcement Learning?"},{"metadata":{},"cell_type":"markdown","source":"### Get the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"csvPath = \"/kaggle/input/california-housing-prices/housing.csv\"\nhousing = pd.read_csv(csvPath)\nhousing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"ocean_proximity\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.hist(bins=150, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The median income is a very important attribute to predict median housing prices. Wem may want to ensure that the test set is representative of the various categories of incomes in the whole dataset. Since the median income is a continuous numerical attribute, first we need to create an income category attribute"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"housing_median_age\"].hist(bins=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"households\"].hist(bins=150)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\nlabels=[1, 2, 3, 4, 5])\nhousing[\"income_cat\"].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainSet, testSet = train_test_split(housing, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) \nfor trainIndex, testIndex in split.split(housing, housing[\"income_cat\"]):\n        stratTrainSet = housing.loc[trainIndex]\n        stratTestSet = housing.loc[testIndex]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testSet[\"income_cat\"].value_counts() / len(stratTestSet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stratTestSet[\"income_cat\"].value_counts() / len(stratTestSet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"].value_counts() / len(housing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize the Data to Gain Insights"},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing Geographical Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Put the test set aside and only explore the training set.\nhousing = stratTrainSet.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(\n    kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrMatrix = housing.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrMatrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## The most promising attribute to predict the median house value is the median income\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n                 alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- First, the correlation is very strong\n- Second, the price cap is clearly visible as a horizontal line at 500000   \n- A horizontal line around 450000, another around 350000\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Try out various attribute combinations\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrMatrix = housing.corr()\ncorrMatrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = stratTrainSet.drop(\"median_house_value\", axis=1)\nhousingLabels = stratTrainSet[\"median_house_value\"].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.dropna(subset=[\"total_bedrooms\"]) # option 1 \nhousing.drop(\"total_bedrooms\", axis=1) # option 2 \nmedian = housing[\"total_bedrooms\"].median() # option 3 \nhousing[\"total_bedrooms\"].fillna(median, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scikit-Learn provides a handy class to take care of missing values: _SimpleImputer_"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer \nimputer = SimpleImputer(strategy=\"median\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## The median can only be computed on numerical attributes, \n## we need to create a copy of the data without the text attribute ocean_proximity:\nhousingNum = housing.drop(\"ocean_proximity\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer.fit(housingNum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housingNum.median().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Use this imputer to transform the training set by replacing missing values by the learned medians\nX = imputer.transform(housingNum)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling Text and Categorical Attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"housingCat = housing[[\"ocean_proximity\"]]\nhousingCat.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert these categories from text to numbers\nfrom sklearn.preprocessing import OrdinalEncoder\nordinalEncoder = OrdinalEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housingCatEncoded = ordinalEncoder.fit_transform(housingCat)\nhousingCatEncoded[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ordinalEncoder.categories_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housingCat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\ncatEncoder = OneHotEncoder()\nhousingCatOnehot = catEncoder.fit_transform(housingCat)\nhousingCatOnehot\n#  the output is a SciPy sparse matrix, instead of a NumPy array.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catEncoder.categories_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Custom Transformers"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room \n    def fit(self, X, y=None):\n        return self # nothing else to do \n    def transform(self, X, y=None):\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix] \n        population_per_household = X[:, population_ix] / X[:, households_ix] \n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n            bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling"},{"metadata":{},"cell_type":"markdown","source":"Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales.  \nThe total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15\n- min-max scaling (normalization): subtracting the min value and dividing by the max minus the min. It has a feature_range hyperparameter that lets you change the range if you don’t want 0–1 for some reason. _MinMaxScaler_   \n- standardization: subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance. _StandardScaler_  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nnumPipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\nhousingNumTr = numPipeline.fit_transform(housingNum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer \nnumAttribs = list(housingNum)\ncatAttribs = [\"ocean_proximity\"]\nfullPipeline = ColumnTransformer([\n     (\"num\", numPipeline, numAttribs),\n     (\"cat\", OneHotEncoder(), catAttribs),\n ])\nhousingPrepared = fullPipeline.fit_transform(housing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression \nlinReg = LinearRegression()\nlinReg.fit(housingPrepared, housingLabels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"someData = housing.iloc[:5]\nsomeLabels = housingLabels.iloc[:5]\nsomeDataPrepared = fullPipeline.transform(someData)\nprint(\"Predictions:\", linReg.predict(someDataPrepared))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nhousingPreedictions = linReg.predict(housingPrepared)\nlinMse = mean_squared_error(housingLabels, housingPreedictions) \nlinRmse = np.sqrt(linMse)\nlinRmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Cross-Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparamGrid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n]\nforestReg = RandomForestRegressor()\ngridSearch = GridSearchCV(forestReg, paramGrid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngridSearch.fit(housingPrepared, housingLabels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridSearch.best_params_ \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridSearch.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres = gridSearch.cv_results_\nfor meanScore, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-meanScore), params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = grid_search.best_estimator_.feature_importances_ \nfeature_importances","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensemble Methods"},{"metadata":{},"cell_type":"markdown","source":"### Evaluate Your System on the Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"finalModel = gridSearch.best_estimator_\nXTest = stratTestSet.drop(\"median_house_value\", axis=1)\nyTest = stratTestSet[\"median_house_value\"].copy()\nXTestPrepared = fullPipeline.transform(XTest)\nfinalPredictions = finalModel.predict(XTestPrepared)\nfinalMse = mean_squared_error(yTest, finalPredictions) \nfinalRmse = np.sqrt(finalMse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc=squared_errors.mean(), \n                         scale=stats.sem(squared_errors)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}