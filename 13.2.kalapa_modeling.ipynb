{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import spatial\n",
    "import math\n",
    "from collections import Counter \n",
    "\n",
    "import datetime, time\n",
    "import pickle\n",
    "import re\n",
    "import pytz\n",
    "\n",
    "# SKLEARN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# TENSORFLOW\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.optimizers import TFOptimizer\n",
    "\n",
    "# KERAS\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printRuntime():\n",
    "    print(datetime.datetime.now(pytz.timezone('Asia/Jakarta')).strftime(\"%Y-%m-%d %T\"))\n",
    "    print(\"-\"*19)\n",
    "    \n",
    "printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    "\n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(labels, preds)\n",
    "    return [('gini', gini_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\"eta\": 0.02, \"max_depth\": 4, \"subsample\": 0.9, \n",
    "              # \"tree_method\": \"gpu_hist\",\n",
    "              \"colsample_bytree\": 0.9, \"objective\": \"binary:logistic\", \n",
    "              \"eval_metric\": \"auc\", \"seed\": 99, \"silent\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfITrain = pd.read_pickle(\"./kalapa/itrain_20200217.pickle\", compression=\"bz2\")\n",
    "dfITest = pd.read_pickle(\"./kalapa/itest_20200217.pickle\", compression=\"bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsEmbedCol = [\"FIELD_%d\"%d for d in [7, 9, 13, 39]] + [\"maCv\", \"jobCat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsFieldFt = [c for c in dfITrain.columns \n",
    "             if \"FIELD\" in c \n",
    "             and c not in lsEmbedCol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lsFieldFt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatJob(iStr):\n",
    "    if \"None\" == iStr or \"none\" == iStr or \"nan\" == iStr:\n",
    "        return \"none\"\n",
    "    # Replace all digit\n",
    "    iStr = re.sub(\"\\d\", \"\", iStr)\n",
    "    # Replace cn to cong nhan\n",
    "    iStr = re.sub(r\"^(cnhân|cn)\", \"công nhân \", iStr)\n",
    "    # Replace nv/nv. to nhan vien\n",
    "    iStr = re.sub(r\"(- nv|nv.|nv)\", \"nhân viên \", iStr)\n",
    "    # Replace p. to phó\n",
    "    iStr = re.sub(r\"(^p\\.)\", \"phó \", iStr)\n",
    "    iStr = (iStr.replace(\"-\", \" \")\n",
    "            .replace(\"cty\", \"công ty\")\n",
    "            .replace(\".\", \" \")\n",
    "            .replace(\"(\", \" \").\n",
    "            replace(\")\", \" \")\n",
    "           )\n",
    "    # Return\n",
    "    return iStr\n",
    "print(formatJob(\"phó giám đốc xí nghiệp\"))\n",
    "print(formatJob(\"p. trưởng phòng\"))\n",
    "printRuntime()\n",
    "\n",
    "def splitJobType(iStr):\n",
    "    iStr = formatJob(iStr)\n",
    "    \n",
    "    lsWord = iStr.split()\n",
    "    if len(lsWord) == 0:\n",
    "        return \"none\", \"none\"\n",
    "    splitIdx = 2\n",
    "    if \"phó chánh\" in iStr or (\"phó trưởng\" in iStr \n",
    "                               and \"phó trưởng phòng\" not in iStr\n",
    "                               and \"phó trưởng ban\" not in iStr):\n",
    "        splitIdx = 4\n",
    "    elif (\"phó phòng\" not in iStr) and (lsWord[0] == \"phó\" or lsWord[0] == \"trưởng\"):\n",
    "        splitIdx = 3\n",
    "    jobCat = \" \".join(lsWord[:splitIdx])\n",
    "    jobDesc = \"none\"\n",
    "    if len(lsWord) > splitIdx:\n",
    "        jobDesc = \" \".join(lsWord[splitIdx:])\n",
    "    return jobCat, jobDesc\n",
    "\n",
    "# Test\n",
    "print(splitJobType(\"nhân viên phòng thí nghiệm\"))\n",
    "print(splitJobType(\"nhân viên bảo trì\"))\n",
    "print(splitJobType(\"cn ủi\"))\n",
    "print(splitJobType(\"9782cấp dưỡng\"))\n",
    "print(splitJobType(\"trưởng dây chuyền phòng sản xuất\"))\n",
    "print(splitJobType(\"p. trưởng phòng\"))\n",
    "printRuntime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format maCv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfTrain = dfITrain.copy()\n",
    "pdfTest = dfITest.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "pdfTrain[\"maCv\"] = pdfTrain[\"maCv\"].apply(formatJob)\n",
    "pdfTrain[\"maCv_VECTOR\"] = pdfTrain[\"maCv\"].apply(lambda x: x.split())\n",
    "pdfTrain[\"jobCat\"], pdfTrain[\"jobDesc\"] = zip(*pdfTrain[\"maCv\"].apply(splitJobType))\n",
    "printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "pdfTest[\"maCv\"] = pdfTest[\"maCv\"].apply(formatJob)\n",
    "pdfTest[\"maCv_VECTOR\"] = pdfTest[\"maCv\"].apply(lambda x: x.split())\n",
    "pdfTest[\"jobCat\"], pdfTest[\"jobDesc\"] = zip(*pdfTest[\"maCv\"].apply(splitJobType))\n",
    "printRuntime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed functions F7 F9 F13 F39 maCv jobCat as pre-computed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitF7(f7):\n",
    "    if f7 == \"[]\" or f7 is np.nan:\n",
    "        return [\"na\"]\n",
    "    s = f7.replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"\").split(\",\")\n",
    "    return s\n",
    "\n",
    "def encodeOneHot(row, vocabSize, word2int, verbose=False):\n",
    "    temp = np.zeros(vocabSize)\n",
    "    if verbose:\n",
    "        print(row)\n",
    "    if row in word2int.keys():\n",
    "        dataPointIndex = word2int[row]\n",
    "        temp[dataPointIndex] = 1\n",
    "    return temp\n",
    "\n",
    "def encodeOneHotVector(row, vocabSize, word2int, verbose=False):\n",
    "    temp = np.zeros(vocabSize)\n",
    "    for c in row:\n",
    "        if c in word2int.keys():\n",
    "            temp[word2int[c]] = temp[word2int[c]] + 1\n",
    "    return temp\n",
    "\n",
    "def extractVocabulary(pdf, iCol, isVector=False, vectorFunc=splitF7):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pdf: training part\n",
    "        iCol: column name to be used to extract vocabulary (FIELD_7, maCv)\n",
    "        isVector: \n",
    "        vectorFunc: Function to build vector from iCol\n",
    "    Returns:\n",
    "        vocab: Vocabulary -> encode\n",
    "        word2int: lookup table word2int -> encode\n",
    "        int2word: lookup table int2word -> encode\n",
    "        vocab, word2int, int2word: Vocabulary from iCol column\n",
    "    \"\"\"\n",
    "    vocab = []\n",
    "    word2int = {}\n",
    "    int2word = {}\n",
    "    # Separate field 7 into vector\n",
    "    if isVector:\n",
    "        # Special treatment for FIELD_7\n",
    "        pdf[iCol+\"_VECTOR\"] = pdf[iCol].apply(vectorFunc)\n",
    "        # Build vocabulary\n",
    "        for r in pdf[iCol+\"_VECTOR\"]:\n",
    "            vocab.extend(r)\n",
    "        vocab = list(set(vocab))\n",
    "    else:\n",
    "        # Build vocabulary\n",
    "        pdf.loc[pdf[iCol].isnull(), iCol] = \"None\" # Prevent nan value for key\n",
    "        vocab = list(pdf[iCol].unique())\n",
    "    # Build 1-hot lookup table\n",
    "    vocabSize = len(vocab) # gives the total number of unique words\n",
    "    print(vocab[:5])\n",
    "    print(vocabSize)\n",
    "    for i,word in enumerate(vocab):\n",
    "        word2int[word] = i\n",
    "        int2word[i] = word\n",
    "    # Return\n",
    "    return vocab, word2int, int2word\n",
    "\n",
    "def encodeCol(pdf, iCol, vocab, word2int, isVector=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dfTrain: train part, only encode column based on this dataframe\n",
    "        iCol: column name to be encoded, in vectorized form\n",
    "    Returns:\n",
    "        pdfEncode: pdf with encoded column (1-hot/1-hot sum vector)\n",
    "    \"\"\"\n",
    "    vocabSize = len(vocab)\n",
    "    # apply 1-hot\n",
    "    if isVector:\n",
    "        newColSeries = pdf[iCol].apply(lambda r: encodeOneHotVector(r, vocabSize, word2int))\n",
    "    else:\n",
    "        newColSeries = pdf[iCol].apply(lambda r: encodeOneHot(r, vocabSize, word2int))\n",
    "    # Return\n",
    "    return newColSeries\n",
    "\n",
    "printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIdx(w, word2int):\n",
    "    if w in word2int.keys():\n",
    "        idx = word2int[w] + 1\n",
    "    else:\n",
    "        idx = -1 # Append non value\n",
    "    return idx\n",
    "\n",
    "def concatEmbedVector(r, word2int, weights, maxLength):\n",
    "    # print(r)\n",
    "    lsVec = []\n",
    "    isVector = (maxLength>1)\n",
    "    embeddingDim = len(weights[0])\n",
    "    notFoundItem = np.array([-1]*embeddingDim, dtype=\"float32\")\n",
    "    if isVector:\n",
    "        for w in r:\n",
    "            idx = getIdx(w, word2int)\n",
    "            if -1 == idx:\n",
    "                lsVec.append(notFoundItem)\n",
    "            else:\n",
    "                lsVec.append(weights[idx])\n",
    "        # Padding to maxLength\n",
    "        paddedValue = maxLength - len(lsVec)\n",
    "        for i in range(paddedValue):\n",
    "            lsVec.append(weights[0])\n",
    "    else:\n",
    "        w = r\n",
    "        idx = getIdx(w, word2int)\n",
    "        if -1 == idx:\n",
    "            lsVec.append(notFoundItem)\n",
    "        else:\n",
    "            lsVec.append(weights[idx])\n",
    "    return np.concatenate(lsVec)\n",
    "\n",
    "# Embed 1 new column into embedded_col\n",
    "# Input parameters\n",
    "def embedCol(pdf, vectorCol, onehotCol, vocab, word2int, weights, \n",
    "             maxLength=1, *args):\n",
    "    # vectorCol = \"FIELD_7_VECTOR\"\n",
    "    # vocab = vocabF7\n",
    "    # word2int = word2intF7\n",
    "    # weights = weightsF7\n",
    "\n",
    "    # Processing\n",
    "    # Using sum of all vectors as doc vector\n",
    "    vocabSize = len(vocab)\n",
    "    isVector = (maxLength > 1)\n",
    "    if isVector:\n",
    "        encodeFunc = encodeOneHotVector\n",
    "    else:\n",
    "        encodeFunc = encodeOneHot\n",
    "        \n",
    "    # pdf[onehotCol] = pdf[vectorCol].apply(lambda r: encodeFunc(r, vocabSize, word2int))\n",
    "    # Using concatenated vector\n",
    "    newConcatSeries = pdf[vectorCol].apply(lambda r: concatEmbedVector(r, word2int, weights, maxLength))\n",
    "    # Using sum vector\n",
    "    newDocSeries = pdf[onehotCol].apply(lambda x: np.matmul(x, weights[1:]))\n",
    "    # return: combined of doc vector and concatnated vector\n",
    "    return newDocSeries, newConcatSeries\n",
    "\n",
    "def trainEmbedCol(pdf, vectorCol, lblCol, onehotCol, vocab, word2int, int2word, embeddingDim, \n",
    "                  maxLength=1, *args):\n",
    "    \"\"\"\n",
    "    Build an embedding model \n",
    "    Args:\n",
    "        pdf:\n",
    "        vectorCol:\n",
    "        lblCol:\n",
    "        vocab:\n",
    "        word2int:\n",
    "        int2word:\n",
    "        embeddingDim:\n",
    "        maxLength: If input is vector -> maxLength := paddedLength; Else 1;\n",
    "    Returns:\n",
    "        weights: Weight matrix (vocabSz*embeddingDim)\n",
    "    \"\"\"\n",
    "    docs = pdf[vectorCol].values\n",
    "    label = pdf[lblCol].values\n",
    "    vocabSize = len(vocab) + 1  # Reserved for position 0 for padding\n",
    "    if maxLength > 1:\n",
    "        encodedDocs = [[(word2int[c]+1) for c in d] for d in docs]\n",
    "        print(docs[:5])\n",
    "        print(encodedDocs[:5])\n",
    "        paddedDocs = pad_sequences(encodedDocs, maxlen=maxLength, padding='post')\n",
    "    else:\n",
    "        encodedDocs = [word2int[d]+1 for d in docs]\n",
    "        print(docs[:5])\n",
    "        print(encodedDocs[:5])\n",
    "        paddedDocs = encodedDocs\n",
    "    print(paddedDocs[:5])\n",
    "    \n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabSize, embeddingDim, input_length=maxLength))\n",
    "    model.add(Flatten(input_shape=(vocabSize, maxLength)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(paddedDocs, label, epochs=100, verbose=0)\n",
    "    \n",
    "    # review auc\n",
    "    yPredTrain = model.predict_proba(paddedDocs)\n",
    "    rocTrain = roc_auc_score(label, yPredTrain)\n",
    "    print(\"-\"*50)\n",
    "    print(\"ROC:\", rocTrain)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # output weight\n",
    "    e = model.layers[0]\n",
    "    weights = e.get_weights()[0]\n",
    "    print(weights.shape) # shape: (vocabSize, embeddingDim)\n",
    "    \n",
    "    embeddedDocSeries, embeddedConcatSeries = embedCol(\n",
    "        pdf, vectorCol, onehotCol, vocab, word2int, weights, maxLength)\n",
    "    # pdf[onehotCol].apply(lambda x: np.matmul(x, weights[1:]))\n",
    "    \n",
    "    # return\n",
    "    return model, weights, embeddedDocSeries, embeddedConcatSeries\n",
    "\n",
    "printRuntime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissionType = [\"max\", \"mean\"]\n",
    "def exportSubmission(idTest, lsResult, giniThreshold, submissionType):\n",
    "    lsChosen = [c for c in lsResult if c[0].best_score > 0.35]\n",
    "    print(len(lsChosen))\n",
    "    submitId = idTest\n",
    "    if \"max\" == submissionType:\n",
    "        maxGini = 0\n",
    "        maxXgbPred = None\n",
    "        for c in lsChosen:\n",
    "            if c[0].best_score > maxGini:\n",
    "                maxGini = c[0].best_score\n",
    "                maxXgbPred = c[1]\n",
    "        print(maxGini)\n",
    "        submitPred = maxXgbPred\n",
    "    elif \"mean\" == submissionType:\n",
    "        submitPred = np.mean([c[1] for c in lsChosen], axis=0)\n",
    "    print(\"Shape of submitId: {}; submitPred: {}\".format(submitId.shape, submitPred.shape))\n",
    "    print(\"Similarity of submission with other chosen pred\")\n",
    "    for c in lsChosen:\n",
    "        print(\"Gini:\", c[0].best_score)\n",
    "        result = 1 - spatial.distance.cosine(c[1], submitPred)\n",
    "        print(\"Similarity:\", result)\n",
    "        \n",
    "    # Write id, maxXgbPred into csv file\n",
    "    dictSubmit = {\"id\": submitId, \"label\": submitPred}\n",
    "    pdfSubmit = pd.DataFrame.from_dict(dictSubmit)\n",
    "    print(pdfSubmit.shape)\n",
    "    pdfSubmit.head()\n",
    "    ymd = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "    oPath = \"/kaggle/working/submission_{}_{}.csv\".format(submissionType, ymd)\n",
    "    print(oPath)\n",
    "    pdfSubmit.to_csv(oPath, header=True, index=False)\n",
    "    # Return\n",
    "    return pdfSubmit\n",
    "\n",
    "printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summaryOutput(pdfSubmit):\n",
    "    pdfSubmit[\"bool_label\"] = (pdfSubmit[\"label\"] > 0.5)\n",
    "    pdfSummary = (pdfSubmit.groupby(\"bool_label\", as_index=False)\n",
    "                  .agg({\"id\": \"count\", \"label\": [\"sum\", \"mean\", \"std\"]})\n",
    "                 )\n",
    "    display(pdfSummary)\n",
    "    printRuntime()\n",
    "    return\n",
    "printRuntime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 0: k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv(\"../input/train.csv\") # train\n",
    "# df_test = pd.read_csv(\"../input/test.csv\") # test\n",
    "K = 5\n",
    "kf = KFold(n_splits = K, random_state = 42, shuffle = True)\n",
    "\n",
    "yTrain = dfITrain[\"label\"].values\n",
    "idTest = dfITest[\"id\"].values\n",
    "\n",
    "xTrain = np.array(dfITrain[lsFieldFt]) # only select field_ft column\n",
    "# test = np.array(df_test.drop([\"id\"], axis = 1))\n",
    "\n",
    "xgb_preds = []\n",
    "printRuntime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOP 17: GINI = 0.21642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "trainVal = []\n",
    "for trainIdx, testIdx in kf.split(xTrain):\n",
    "    pdfTrain, pdfTest = dfITrain.iloc[trainIdx].copy(), dfITrain.iloc[testIdx].copy()\n",
    "    print(pdfTrain.shape, pdfTest.shape)\n",
    "    # Embedding field 7, 9, 13, 39\n",
    "    print(\"Doing embedding ...\")\n",
    "    lsEmbeddedCol = []\n",
    "    for col in lsEmbedCol:\n",
    "        print(\"#\", col)\n",
    "        t = time.time()\n",
    "        onehotCol = col + \"_ONEHOT\"\n",
    "        embeddedCol = col + \"_EMBEDDED\"\n",
    "        vocab, word2int, int2word, pdfTrain[onehotCol] = encodeCol(pdfTrain, col)\n",
    "        embeddingDim = 2\n",
    "        maxLength = 1\n",
    "        if \"FIELD_7\" == col:\n",
    "            iCol = \"FIELD_7_VECTOR\"\n",
    "            embeddingDim = 2\n",
    "            maxLength = 16\n",
    "        else:\n",
    "            iCol = col\n",
    "        model, weights, pdfTrain[embeddedCol] = trainEmbedCol(pdfTrain, iCol, \"label\", onehotCol,\n",
    "                                                           vocab, word2int, int2word, embeddingDim, maxLength)\n",
    "        pdfTest[embeddedCol] = embedCol(pdfTest, col, onehotCol, vocab, word2int, weights)\n",
    "\n",
    "        # pickleVal[col] = [vocab, word2int, int2word, embeddingDim, maxLength, weights]\n",
    "\n",
    "        embeddingDim = weights.shape[1]\n",
    "        lsCol = [(embeddedCol + \"_%d\"%d) for d in range(embeddingDim)]\n",
    "        lsEmbeddedCol.extend(lsCol)\n",
    "        pdfTrain[lsCol] = pd.DataFrame(pdfTrain[embeddedCol].values.tolist(), index=pdfTrain.index)\n",
    "        pdfTest[lsCol] = pd.DataFrame(pdfTest[embeddedCol].values.tolist(), index=pdfTest.index)\n",
    "        print(\"Elapsed time: %d(s)\" % int(time.time() - t))\n",
    "        printRuntime()\n",
    "    \n",
    "    # params configuration also from the1owl's kernel\n",
    "    # https://www.kaggle.com/the1owl/forza-baseline\n",
    "    train_X, valid_X = pdfTrain[lsFieldFt+lsEmbeddedCol].values, pdfTest[lsFieldFt+lsEmbeddedCol].values\n",
    "    train_y, valid_y = yTrain[trainIdx], yTrain[testIdx]\n",
    "    \n",
    "    d_train = xgb.DMatrix(train_X, train_y)\n",
    "    d_valid = xgb.DMatrix(valid_X, valid_y)\n",
    "    trainVal.append((d_train, d_valid))\n",
    "printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (d_train, d_valid) in trainVal:\n",
    "    # d_test = xgb.DMatrix(test)\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    model = xgb.train(xgb_params, d_train, 5000,  watchlist, feval=gini_xgb, maximize=True, \n",
    "                      verbose_eval=50, early_stopping_rounds=200)\n",
    "                        \n",
    "    # xgb_pred = model.predict(d_test)\n",
    "    # xgb_preds.append(list(xgb_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild pdfTrain pdfTest based on precalculated embeddedCol\n",
    "print(\"Load pre-trained parameters to pickle file\")\n",
    "path = \"../input/kalapa/weight.pickle\"\n",
    "print(path)\n",
    "with open(path, 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in b.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsEmbeddedCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsEmbeddedCol = []\n",
    "print(\"Reload embedding value...\")\n",
    "pdfTrain = dfITrain\n",
    "pdfTest = dfITest\n",
    "for col in lsEmbedCol:\n",
    "    print(\"#\", col)\n",
    "    t = time.time()\n",
    "    onehotCol = col + \"_ONEHOT\"\n",
    "    # vectorCol = col + \"_VECTOR\"\n",
    "    embeddedCol = col + \"_EMBEDDED\"\n",
    "    embeddedDocsCol = col + \"_DOCS\"\n",
    "    embeddedConcatCol = col + \"_CONCAT\"\n",
    "    \n",
    "    [vocab, word2int, int2word, embeddingDim, maxLength, weights] = b[col]\n",
    "    isVector = (maxLength > 1)\n",
    "    \n",
    "    if isVector:\n",
    "        print(\"Vectorize column\", col)\n",
    "        pdfTrain[vectorCol] = pdfTrain[col].apply(vectorFunc)\n",
    "        pdfTest[vectorCol] = pdfTest[col].apply(vectorFunc)\n",
    "        print(\"-\"*20)\n",
    "    print(\"One-hot encoding column\", col)\n",
    "    pdfTrain[onehotCol] = encodeCol(pdfTrain, vectorCol, vocab, word2int, isVector)\n",
    "    pdfTest[onehotCol] = encodeCol(pdfTest, vectorCol, vocab, word2int, isVector)\n",
    "    \n",
    "    print(\"Embed column {} into docs&concat\".format(col))\n",
    "    pdfTrain[embeddedDocsCol], pdfTrain[embeddedConcatCol] = embedCol(\n",
    "        pdfTrain, vectorCol, onehotCol, vocab, word2int, weights, maxLength)\n",
    "    pdfTest[embeddedDocsCol], pdfTest[embeddedConcatCol] = embedCol(\n",
    "        pdfTest, vectorCol, onehotCol, vocab, word2int, weights, maxLength)\n",
    "    print(\"-\"*20)\n",
    "    \n",
    "    print(\"Stretch out column {}\".format(embeddedDocsCol))\n",
    "    lsDocsCol = [(embeddedDocsCol + \"_%d\"%d) for d in range(embeddingDim)]\n",
    "    pdfTrain[lsDocsCol] = pd.DataFrame(pdfTrain[embeddedDocsCol].values.tolist(), index=pdfTrain.index)\n",
    "    pdfTest[lsDocsCol] = pd.DataFrame(pdfTest[embeddedDocsCol].values.tolist(), index=pdfTest.index)\n",
    "    print(\"-\"*20)\n",
    "    # Stretch out concat col\n",
    "    if isVector:\n",
    "        print(\"Stretch out column {}\".format(embeddedConcatCol))\n",
    "        lsConcatCol = [(embeddedConcatCol + \"_%d\"%d) for d in range(embeddingDim*maxLength)]\n",
    "        pdfTrain[lsConcatCol] = pd.DataFrame(pdfTrain[embeddedConcatCol].values.tolist(), index=pdfTrain.index)\n",
    "        pdfTest[lsConcatCol] = pd.DataFrame(pdfTest[embeddedConcatCol].values.tolist(), index=pdfTest.index)\n",
    "        print(\"-\"*20)\n",
    "        \n",
    "    print(\"Elapsed time: %d(s)\" % int(time.time() - t))\n",
    "    printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsFieldFt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEADER 0.34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN 01: GINI 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 1 model with full data and use to predict test\n",
    "# Random split train_test\n",
    "# xTrain = np.array(dfITrain[lsFieldFt]) # only select field_ft column\n",
    "NUM_RUN = 50\n",
    "xTrain = pdfTrain[lsFieldFt+lsEmbeddedCol].values\n",
    "xTest = pdfTest[lsFieldFt+lsEmbeddedCol].values\n",
    "\n",
    "yTrain = dfITrain[\"label\"].values\n",
    "lsResult = []\n",
    "for i in range(NUM_RUN):\n",
    "    t = time.time()\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(xTrain, yTrain, test_size=0.2)\n",
    "    print(train_X.shape)\n",
    "    print(valid_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(valid_y.shape)\n",
    "\n",
    "    # train_y, valid_y = yTrain[trainIdx], yTrain[testIdx]\n",
    "    d_train = xgb.DMatrix(train_X, train_y)\n",
    "    d_valid = xgb.DMatrix(valid_X, valid_y)\n",
    "    # d_test = xgb.DMatrix(test)\n",
    "\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    model = xgb.train(xgb_params, d_train, 5000,  watchlist, feval=gini_xgb, maximize=True, \n",
    "                          verbose_eval=50, early_stopping_rounds=100)\n",
    "    \n",
    "    d_test = xgb.DMatrix(xTest)\n",
    "    xgb_pred = model.predict(d_test)\n",
    "    print(xgb_pred[:5])\n",
    "    lsResult.append((model, xgb_pred))\n",
    "    print(\"Elapsed time: %d(s)\" % int(time.time() - t))\n",
    "    printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try train/test 0.9/0.1\n",
    "NUM_RUN = 50\n",
    "xTrain = pdfTrain[lsFieldFt+lsEmbeddedCol].values\n",
    "xTest = pdfTest[lsFieldFt+lsEmbeddedCol].values\n",
    "\n",
    "yTrain = dfITrain[\"label\"].values\n",
    "# lsResult = []\n",
    "for i in range(NUM_RUN):\n",
    "    t = time.time()\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(xTrain, yTrain, test_size=0.1)\n",
    "    print(train_X.shape)\n",
    "    print(valid_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(valid_y.shape)\n",
    "\n",
    "    # train_y, valid_y = yTrain[trainIdx], yTrain[testIdx]\n",
    "    d_train = xgb.DMatrix(train_X, train_y)\n",
    "    d_valid = xgb.DMatrix(valid_X, valid_y)\n",
    "    # d_test = xgb.DMatrix(test)\n",
    "\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    model = xgb.train(xgb_params, d_train, 5000,  watchlist, feval=gini_xgb, maximize=True, \n",
    "                          verbose_eval=50, early_stopping_rounds=100)\n",
    "    \n",
    "    d_test = xgb.DMatrix(xTest)\n",
    "    xgb_pred = model.predict(d_test)\n",
    "    print(xgb_pred[:5])\n",
    "    lsResult.append((model, xgb_pred))\n",
    "    print(\"Elapsed time: %d(s)\" % int(time.time() - t))\n",
    "    printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performance model => Combine score of top 10 models\n",
    "maxGini = 0\n",
    "maxXgbPred = None\n",
    "idTest = pdfTest[\"id\"].values\n",
    "for c in lsResult:\n",
    "    if c[0].best_score > maxGini:\n",
    "        maxGini = c[0].best_score\n",
    "        maxXgbPred = c[1]\n",
    "print(maxGini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsChosen = [c for c in lsResult if c[0].best_score > 0.35]\n",
    "len(lsChosen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitId = idTest\n",
    "# axis=0 => (20000,)\n",
    "# axis=1 => (12, ) => mean of each vector\n",
    "submitPred = np.mean([c[1] for c in lsChosen], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitPred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cosine similarity between mean vector and each vector\n",
    "for c in lsChosen:\n",
    "    print(\"Gini:\", c[0].best_score)\n",
    "    result = 1 - spatial.distance.cosine(c[1], submitPred)\n",
    "    print(\"Similarity:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write id, maxXgbPred into csv file\n",
    "dictSubmit = {\"id\": submitId, \"label\": submitPred}\n",
    "pdfSubmit = pd.DataFrame.from_dict(dictSubmit)\n",
    "print(pdfSubmit.shape)\n",
    "pdfSubmit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymd = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "pdfSubmit.to_csv(\"/kaggle/working/submission_{}.csv\".format(ymd), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN 2: Not using embedded F7 F9 F13 F39, F_mean, F_std\n",
    "# RUN 3: Not using embedded F7 F9 F13 F39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submission_max: 0.406523, result: 0.16533\n",
    "submission_mean: result: 0.17629\n",
    "=> BAD: try mean with embedded fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfTrain = dfITrain.copy()\n",
    "pdfTest = dfITest.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsSelectedFt = ([c for c in lsFieldFt if \"_mean\" not in c and \"_std\" not in c] \n",
    "#                 + [c for c in lsFieldFt if \"_stdized\" in c]) # LOW: 0.17629\n",
    "# lsSelectedFt = ([c for c in lsFieldFt if \"_mean\" not in c and \"_std\" not in c] \n",
    "#                 + [c for c in lsFieldFt if \"_stdized\" in c]) + lsEmbeddedCol # LOW: 0.16358\n",
    "lsSelectedFt = lsFieldFt\n",
    "# print(lsSelectedFt)\n",
    "printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 1 model with full data and use to predict test\n",
    "# Random split train_test\n",
    "# xTrain = np.array(dfITrain[lsSelectedFt]) # only select field_ft column\n",
    "NUM_RUN = 50\n",
    "xTrain = pdfTrain[lsSelectedFt].values\n",
    "xTest = pdfTest[lsSelectedFt].values\n",
    "\n",
    "yTrain = pdfTrain[\"label\"].values\n",
    "lsResult = []\n",
    "for i in range(NUM_RUN):\n",
    "    t = time.time()\n",
    "    pdfTrainX, pdfValidateX = train_test_split(pdfTrain, test_size=0.1)\n",
    "    train_X, train_y = pdfTrainX[lsSelectedFt].values, pdfTrainX[\"label\"].values\n",
    "    valid_X, valid_y = pdfValidateX[lsSelectedFt].values, pdfValidateX[\"label\"].values\n",
    "    print(train_X.shape)\n",
    "    print(valid_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(valid_y.shape)\n",
    "    display(pdfTrainX.groupby(\"label\").agg({\"id\": \"count\"}))\n",
    "    display(pdfValidateX.groupby(\"label\").agg({\"id\": \"count\"}))\n",
    "\n",
    "    # train_y, valid_y = yTrain[trainIdx], yTrain[testIdx]\n",
    "    d_train = xgb.DMatrix(train_X, train_y)\n",
    "    d_valid = xgb.DMatrix(valid_X, valid_y)\n",
    "    # d_test = xgb.DMatrix(test)\n",
    "\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    model = xgb.train(xgb_params, d_train, 5000,  watchlist, feval=gini_xgb, maximize=True, \n",
    "                          verbose_eval=50, early_stopping_rounds=100)\n",
    "    \n",
    "    d_test = xgb.DMatrix(xTest)\n",
    "    xgb_pred = model.predict(d_test)\n",
    "    print(xgb_pred[:5])\n",
    "    lsResult.append((model, xgb_pred))\n",
    "    print(\"Elapsed time: %d(s)\" % int(time.time() - t))\n",
    "    printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try train/test 0.9/0.1 => Target encoding and embedding must be done inside 1 loop\n",
    "NUM_RUN = 50\n",
    "xTrain = pdfTrain[lsSelectedFt].values\n",
    "xTest = pdfTest[lsSelectedFt].values\n",
    "\n",
    "yTrain = pdfTrain[\"label\"].values\n",
    "# lsResult = []\n",
    "for i in range(NUM_RUN):\n",
    "    t = time.time()\n",
    "    pdfTrainX, pdfValidateX = train_test_split(pdfTrain, test_size=0.1)\n",
    "    train_X, train_y = pdfTrainX[lsSelectedFt].values, pdfTrainX[\"label\"].values\n",
    "    valid_X, valid_y = pdfValidateX[lsSelectedFt].values, pdfValidateX[\"label\"].values\n",
    "    print(train_X.shape)\n",
    "    print(valid_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(valid_y.shape)\n",
    "    display(pdfTrainX.groupby(\"label\").agg({\"id\": \"count\"}))\n",
    "    display(pdfValidateX.groupby(\"label\").agg({\"id\": \"count\"}))\n",
    "\n",
    "    # train_y, valid_y = yTrain[trainIdx], yTrain[testIdx]\n",
    "    d_train = xgb.DMatrix(train_X, train_y)\n",
    "    d_valid = xgb.DMatrix(valid_X, valid_y)\n",
    "    # d_test = xgb.DMatrix(test)\n",
    "\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    model = xgb.train(xgb_params, d_train, 5000,  watchlist, feval=gini_xgb, maximize=True, \n",
    "                          verbose_eval=50, early_stopping_rounds=100)\n",
    "    \n",
    "    d_test = xgb.DMatrix(xTest)\n",
    "    xgb_pred = model.predict(d_test)\n",
    "    print(xgb_pred[:5])\n",
    "    lsResult.append((model, xgb_pred))\n",
    "    print(\"Elapsed time: %d(s)\" % int(time.time() - t))\n",
    "    printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the giniThreshold\n",
    "giniThreshold = 0.372\n",
    "idTest = pdfTest[\"id\"].values\n",
    "lsChosen = [c for c in lsResult if c[0].best_score > giniThreshold]\n",
    "len(lsChosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the distribution of pdfSubmit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfSubmitSum = exportSubmission(idTest, lsResult, giniThreshold=0.35, submissionType=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryOutput(pdfSubmitSum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfSubmitMean = exportSubmission(idTest, lsResult, giniThreshold=0.35, submissionType=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryOutput(pdfSubmitMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN 4: Embedding inside 1 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsCol = [8, 10, 12, 17, 24, 40, 43] # For target encoding\n",
    "lsMeanEncodCol = [\"FIELD_%d\"%d for d in lsCol] + [\"group_age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsEmbedCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsFieldFt = [c for c in pdfTrain.columns \n",
    "             if \"FIELD\" in c \n",
    "             and \"_ecd\" not in c\n",
    "             and c not in lsMeanEncodCol \n",
    "             and c not in lsEmbedCol]\n",
    "print(len(lsFieldFt))\n",
    "printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsFieldFt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfTrainBk = pdfTrain.copy()\n",
    "pdfTestBk = pdfTest.copy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pdfTrain = pdfTrainBk.copy()\n",
    "pdfTest = pdfTestBk.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEmbedding(pdfTrain, pdfVal, pdfTest, lsEmbedCol):\n",
    "    print(\"# --- Doing embedding --- #\")\n",
    "    # pickleVal[i] = {}\n",
    "    lsEmbeddedCol = []\n",
    "    for col in lsEmbedCol:\n",
    "        print(\"#\", col)\n",
    "        t = time.time()\n",
    "        embeddingDim = 2\n",
    "        maxLength = 1\n",
    "        if \"FIELD_7\" == col:\n",
    "            vectorCol = \"FIELD_7_VECTOR\"\n",
    "            maxLength = 16\n",
    "            vectorFunc = splitF7\n",
    "        elif \"maCv\" == col:\n",
    "            vectorCol = \"maCv_VECTOR\"\n",
    "            maxLength = 42\n",
    "            vectorFunc = lambda x: x.split()\n",
    "        else:\n",
    "            vectorCol = col\n",
    "        isVector = (maxLength > 1)    \n",
    "\n",
    "        onehotCol = col + \"_ONEHOT\"\n",
    "        embeddedCol = col + \"_EMBEDDED\"\n",
    "        embeddedDocsCol = col + \"_DOCS\"\n",
    "        embeddedConcatCol = col + \"_CONCAT\"\n",
    "        print(\"# extract vocabulary\")\n",
    "        vocab, word2int, int2word = extractVocabulary(pdfTrain, col, isVector, vectorFunc)\n",
    "        print(\"# one-hot encoding\")\n",
    "        pdfTrain[onehotCol] = encodeCol(pdfTrain, vectorCol, vocab, word2int, isVector)\n",
    "        if isVector:\n",
    "            pdfVal[vectorCol] = pdfVal[col].apply(vectorFunc)\n",
    "            pdfTest[vectorCol] = pdfTest[col].apply(vectorFunc)\n",
    "        pdfVal[onehotCol] = encodeCol(pdfVal, vectorCol, vocab, word2int, isVector)\n",
    "        pdfTest[onehotCol] = encodeCol(pdfTest, vectorCol, vocab, word2int, isVector)\n",
    "        \n",
    "        # Train the embedding\n",
    "        print(\"# Train the embedding\")\n",
    "        model, weights, pdfTrain[embeddedDocsCol], pdfTrain[embeddedConcatCol] = trainEmbedCol(\n",
    "            pdfTrain, vectorCol, \"label\", onehotCol,\n",
    "            vocab, word2int, int2word, embeddingDim, maxLength, isVector, vectorFunc)\n",
    "        # Embed validate, test \n",
    "        pdfVal[embeddedDocsCol], pdfVal[embeddedConcatCol] = embedCol(\n",
    "            pdfVal, vectorCol, onehotCol, vocab, word2int, weights, maxLength)\n",
    "        \n",
    "        pdfTest[embeddedDocsCol], pdfTest[embeddedConcatCol] = embedCol(\n",
    "            pdfTest, vectorCol, onehotCol, vocab, word2int, weights, maxLength)\n",
    "\n",
    "        # Save the weights and vocab to file\n",
    "        # pickleVal[i][col] = [vocab, word2int, int2word, embeddingDim, maxLength, weights]\n",
    "        # Stretch out doc col\n",
    "        # embeddingDim = weights.shape[1]\n",
    "        lsDocsCol = [(embeddedDocsCol + \"_%d\"%d) for d in range(embeddingDim)]\n",
    "        pdfTrain[lsDocsCol] = pd.DataFrame(pdfTrain[embeddedDocsCol].values.tolist(), \n",
    "                                            index=pdfTrain.index)\n",
    "        pdfVal[lsDocsCol] = pd.DataFrame(pdfVal[embeddedDocsCol].values.tolist(), \n",
    "                                               index=pdfVal.index)\n",
    "        pdfTest[lsDocsCol] = pd.DataFrame(pdfTest[embeddedDocsCol].values.tolist(), \n",
    "                                          index=pdfTest.index)\n",
    "        lsEmbeddedCol.extend(lsDocsCol)\n",
    "        # Stretch out concat col\n",
    "        if isVector:\n",
    "            lsConcatCol = [(embeddedConcatCol + \"_%d\"%d) for d in range(embeddingDim*maxLength)]\n",
    "            pdfTrain[lsConcatCol] = pd.DataFrame(pdfTrain[embeddedConcatCol].values.tolist(), \n",
    "                                                  index=pdfTrain.index)\n",
    "            pdfVal[lsConcatCol] = pd.DataFrame(pdfVal[embeddedConcatCol].values.tolist(), \n",
    "                                                     index=pdfVal.index)\n",
    "            pdfTest[lsConcatCol] = pd.DataFrame(pdfTest[embeddedConcatCol].values.tolist(), \n",
    "                                                index=pdfTest.index)\n",
    "            lsEmbeddedCol.extend(lsConcatCol)\n",
    "\n",
    "        print(\"Elapsed time: %d(s)\" % int(time.time() - t))\n",
    "    return pdfTrain, pdfVal, pdfTest, lsEmbeddedCol\n",
    "printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTargetEncoding(pdfTrain, pdfVal, pdfTest, lsMeanEncodCol):\n",
    "    print(\"Target encoding for train: {}\".format(lsMeanEncodCol))\n",
    "    meanEcd = {}\n",
    "    lsEmbeddedCol = []\n",
    "    for cName in lsMeanEncodCol:\n",
    "        pdfTrain[cName].replace(np.nan, \"none\")\n",
    "        meanEcd[cName] = pdfTrain.groupby([cName], as_index=False).agg({\"id\":\"count\", \"label\":[\"mean\", \"std\"]})\n",
    "        meanEcd[cName].columns = [\"_\".join(x) for x in meanEcd[cName].columns.ravel()]\n",
    "        meanEcd[cName] = meanEcd[cName].rename(columns={\n",
    "            cName+\"_\": cName, \"id_count\": cName+\"_ecdcount\",\n",
    "            \"label_mean\": cName+\"_ecdmean\", \"label_std\": cName+\"_ecdstd\"})\n",
    "        pdfTrain = pd.merge(pdfTrain, meanEcd[cName], on=cName, how=\"left\")\n",
    "        lsEmbeddedCol.extend([cName+\"_ecdcount\", cName+\"_ecdmean\", cName+\"_ecdstd\"])\n",
    "#         pickleVal[i][cName] = meanEcd[cName].to_dict()\n",
    "    # Get target/mean encoding of columns B in pdfVal & pdfTest\n",
    "    print(\"Target encoding for Val/Test using train data: {}\".format(lsMeanEncodCol))\n",
    "    for cName in lsMeanEncodCol:\n",
    "        # Val\n",
    "        pdfVal[cName].replace(np.nan, \"none\")\n",
    "        pdfVal = pd.merge(pdfVal, meanEcd[cName], on=cName, how=\"left\")\n",
    "        # TestX\n",
    "        pdfTest[cName].replace(np.nan, \"none\")\n",
    "        pdfTest = pd.merge(pdfTest, meanEcd[cName], on=cName, how=\"left\")\n",
    "    return pdfTrain, pdfVal, pdfTest, lsEmbeddedCol\n",
    "printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back-up the data\n",
    "# pdfTrain = dfITrain.copy()\n",
    "# pdfTestX = dfITest.copy()\n",
    "\n",
    "# Try train/test 0.9/0.1 => Target encoding and embedding must be done inside 1 loop\n",
    "NUM_RUN = 30\n",
    "pickleVal = {}\n",
    "lsResult = []\n",
    "\n",
    "for i in range(NUM_RUN):\n",
    "    t = time.time()\n",
    "    train, val = train_test_split(pdfTrain, test_size=0.2)\n",
    "    pdfTrainX = train.copy()\n",
    "    pdfValidateX = val.copy()\n",
    "    pdfTestX = pdfTest.copy()\n",
    "    lsEmbeddedCol = []\n",
    "#     pdfTrainX, pdfValidateX, pdfTestX, lsNewCol = runEmbedding(\n",
    "#         pdfTrainX, pdfValidateX, pdfTestX, lsEmbedCol)\n",
    "#     lsEmbeddedCol.extend(lsNewCol)\n",
    "#     print(\"Shape after embedding {}\".format(lsEmbedCol))\n",
    "#     print(pdfTrainX.shape, pdfValidateX.shape, pdfTestX.shape)\n",
    "#     print(\"=\"*50)\n",
    "    # ----- # \n",
    "    pdfTrainX, pdfValidateX, pdfTestX, lsNewCol = runTargetEncoding(\n",
    "        pdfTrainX, pdfValidateX, pdfTestX, lsMeanEncodCol)\n",
    "    lsEmbeddedCol.extend(lsNewCol)\n",
    "    print(\"Shape after target-encoding {}\".format(lsMeanEncodCol))\n",
    "    print(pdfTrainX.shape, pdfValidateX.shape, pdfTestX.shape)\n",
    "    printRuntime()\n",
    "    print(\"=\"*50)\n",
    "    # ----- #\n",
    "    \n",
    "    lsSelectedFt = lsFieldFt + lsEmbeddedCol\n",
    "    print(\"Train XGBoost model\")\n",
    "    train_X, train_y = pdfTrainX[lsSelectedFt].values, pdfTrainX[\"label\"].values\n",
    "    valid_X, valid_y = pdfValidateX[lsSelectedFt].values, pdfValidateX[\"label\"].values\n",
    "    print(train_X.shape)\n",
    "    print(valid_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(valid_y.shape)\n",
    "    display(pdfTrainX.groupby(\"label\").agg({\"id\": \"count\"}))\n",
    "    display(pdfValidateX.groupby(\"label\").agg({\"id\": \"count\"}))\n",
    "\n",
    "    # train_y, valid_y = yTrain[trainIdx], yTrain[testIdx]\n",
    "    d_train = xgb.DMatrix(train_X, train_y)\n",
    "    d_valid = xgb.DMatrix(valid_X, valid_y)\n",
    "    # d_test = xgb.DMatrix(test)\n",
    "\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    model = xgb.train(xgb_params, d_train, 5000,  watchlist, feval=gini_xgb, maximize=True, \n",
    "                          verbose_eval=50, early_stopping_rounds=100)\n",
    "    \n",
    "    xTest = pdfTestX[lsSelectedFt].values\n",
    "    d_test = xgb.DMatrix(xTest)\n",
    "    xgb_pred = model.predict(d_test)\n",
    "    print(xgb_pred[:5])\n",
    "    lsResult.append((model, xgb_pred))\n",
    "    print(\"Elapsed time: %d(s)\" % int(time.time() - t))\n",
    "    printRuntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the giniThreshold\n",
    "giniThreshold = 0.372\n",
    "idTest = pdfTest[\"id\"].values\n",
    "lsChosen = [c for c in lsResult if c[0].best_score > giniThreshold]\n",
    "len(lsChosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the distribution of pdfSubmit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfSubmitSum = exportSubmission(idTest, lsResult, giniThreshold, submissionType=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryOutput(pdfSubmitSum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfSubmitMean = exportSubmission(idTest, lsResult, giniThreshold, submissionType=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryOutput(pdfSubmitMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Try filtering iteratively"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
