{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "71937b91-6de7-bfcf-0865-0f0157d3bce6"
   },
   "source": [
    "**A Naive Bayes Way for Spam Classification using Tabu List**\n",
    "=============================================================\n",
    "Implemented by Clyde Wang, Feb, 17, 2017\n",
    "\n",
    "This spam classifier is implemented by Naive Bayes Model, a simple but very efficient solution in spam classification problem. In brief, Naive Bayes treats every features independent from each other, making inference very efficient. You can refer to [Naive Bayes][1] for more details. This article briefly introduces the process of the selection of tabu list and building the learning algorithm. I hope you will enjoy it.\n",
    "\n",
    "This code runs quite well with the accuracy of 98.31% on training sample and 97.81% on test sample.\n",
    "I hope someone could improve it and enhance its performance. And Here we go!\n",
    "\n",
    "  [1]: https://en.wikipedia.org/wiki/Naive_Bayes_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f0e046f7-a766-6dfa-f05c-3d8ba49f1e2a"
   },
   "source": [
    "## 1. Environment Setting ##\n",
    "Before we start we need set up the environment, please make sure those packages are installed in your computer when you copy this code to the local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "100e1654-dc9c-3ff7-51b9-4ca782dfbbfc"
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b98cf320-7a0c-a003-dce5-b2f794244212"
   },
   "source": [
    "## 2. Read Data File\n",
    "-----------------\n",
    "The first task is to read data from .csv file, and here we could take advantage of the `pandas.DataFrame` to store and process the raw data. (see [here][1]) Notice that we have to split our raw data into two parts in order to test the generalisation ability of our model. Here is the code:\n",
    "\n",
    "\n",
    "  [1]: http://pandas.pydata.org/pandas-docs/stable/10min.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "2b46f245-3cb2-7cea-6d32-132cf6b7108f"
   },
   "outputs": [],
   "source": [
    "def readData():\n",
    "    SMS_df = pd.read_csv('data/spam.csv',usecols=[0,1],encoding='latin-1')\n",
    "    SMS_df.columns=['label','content']\n",
    "    n = int(SMS_df.shape[0])\n",
    "    # split into training data and test data\n",
    "    return SMS_df.iloc[:int(0.75*n)], SMS_df.iloc[int(n*0.75):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfTrain, pdfTest = readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 2) (1393, 2)\n"
     ]
    }
   ],
   "source": [
    "print(pdfTrain.shape, pdfTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfTrain.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4179</th>\n",
       "      <td>ham</td>\n",
       "      <td>swhrt how u dey,hope ur ok, tot about u 2day.l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4180</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok da, i already planned. I wil pick you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4181</th>\n",
       "      <td>spam</td>\n",
       "      <td>Urgent! Please call 0906346330. Your ABTA comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4182</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later in meeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4183</th>\n",
       "      <td>ham</td>\n",
       "      <td>I just really need shit before tomorrow and I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            content\n",
       "4179   ham  swhrt how u dey,hope ur ok, tot about u 2day.l...\n",
       "4180   ham          Ok da, i already planned. I wil pick you.\n",
       "4181  spam  Urgent! Please call 0906346330. Your ABTA comp...\n",
       "4182   ham                  Sorry, I'll call later in meeting\n",
       "4183   ham  I just really need shit before tomorrow and I ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfTest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b25b31ab-b108-f232-5fc8-47d2449c6b55"
   },
   "source": [
    "## 3. Generate a Tabu List\n",
    "--------------------\n",
    "A **tabu list** is a list of those significant indicators of spam SMS. Here we select TF-IDF as the principle of list generation.\n",
    "\n",
    "Term Frequency(TF) is the frequency of a word in a certain kind of document. If there is a article of 50 words with 2 'data' in it, then the TF of the 'data' is given by `2/50=0.01`.\n",
    "\n",
    "However, there are some words of high frequency in English, like 'a', 'is', 'are', etc. We have to remove those words from our list. And here comes the IDF.\n",
    "\n",
    "Inverse Document Frequency(IDF) is the indicator to reflect how important a word is related to some certain topic. It is given by `log(#total articles/#articles containing w)`, for example, if we have 5 articles, only one has word 'gene' of term frequency of 0.002, but all the five articles contains the word 'technology' of term frequency of 0.5, the IDF of 'gene' is `log(5/1)>0` but  the IDF of 'technology' is `log(5/5)=0`.\n",
    "\n",
    "TF-IDF is the product of TF and IDF: in the example above, `TFIDF('gene')=0.002*log(5/1)>0` while `TFIDF('technology')=0.5*log(5/5)=0`, so in this case, 'gene' is a better indicator than 'technology'.\n",
    "\n",
    "In my code, I compute the TF-IDF of each word for 'spam' and 'ham', and compute the difference between them, thus I can select the words which are the most representative for the 'spam' class.\n",
    "\n",
    "Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "e7a4eada-c2cc-e24f-4c82-39f4ba8739e4"
   },
   "outputs": [],
   "source": [
    "def generate_tabu_list(path, tabu_size=200,ignore=3):\n",
    "    train_df,_ = readData()\n",
    "    spam_TF_dict = dict()\n",
    "    ham_TF_dict = dict()\n",
    "    IDF_dict = dict()\n",
    "\n",
    "    # ignore all other than letters.\n",
    "    for i in range(train_df.shape[0]):\n",
    "        finds = re.findall('[A-Za-z]+', train_df.iloc[i].content)\n",
    "        if train_df.iloc[i].label == 'spam':\n",
    "            for find in finds:\n",
    "                if len(find)<ignore: continue\n",
    "                find = find.lower()\n",
    "                try:\n",
    "                    spam_TF_dict[find] = spam_TF_dict[find] + 1\n",
    "                except:    \n",
    "                    spam_TF_dict[find] = spam_TF_dict.get(find,1)\n",
    "                    ham_TF_dict[find] = ham_TF_dict.get(find,0)\n",
    "        else:\n",
    "            for find in finds:\n",
    "                if len(find)<ignore: continue\n",
    "                find = find.lower()\n",
    "                try:\n",
    "                    ham_TF_dict[find] = ham_TF_dict[find] + 1\n",
    "                except:    \n",
    "                    spam_TF_dict[find] = spam_TF_dict.get(find,0)\n",
    "                    ham_TF_dict[find] = ham_TF_dict.get(find,1)\n",
    "        \n",
    "        word_set = set()\n",
    "        for find in finds:\n",
    "            if len(find)<ignore: continue\n",
    "            find = find.lower()\n",
    "            if not(find in word_set):\n",
    "                try:\n",
    "                    IDF_dict[find] = IDF_dict[find] + 1\n",
    "                except:    \n",
    "                    IDF_dict[find] = IDF_dict.get(find,1)\n",
    "            word_set.add(find)\n",
    "\n",
    "    word_df = pd.DataFrame(list(zip(ham_TF_dict.keys(),ham_TF_dict.values(),spam_TF_dict.values(),IDF_dict.values())))\n",
    "    word_df.columns = ['keyword','ham_TF','spam_TF','IDF']\n",
    "    word_df['ham_TF'] = word_df['ham_TF'].astype('float')/train_df[train_df['label']=='ham'].shape[0]\n",
    "    word_df['spam_TF'] = word_df['spam_TF'].astype('float')/train_df[train_df['label']=='spam'].shape[0]\n",
    "    word_df['IDF'] = np.log10(train_df.shape[0]/word_df['IDF'].astype('float'))\n",
    "    word_df['ham_TFIDF'] = word_df['ham_TF']*word_df['IDF']\n",
    "    word_df['spam_TFIDF'] = word_df['spam_TF']*word_df['IDF']\n",
    "    word_df['diff']=word_df['spam_TFIDF']-word_df['ham_TFIDF']\n",
    "\n",
    "    selected_spam_key = word_df.sort_values('diff',ascending=False)\n",
    "\n",
    "    print('>>>Generating Tabu List...\\n  Tabu List Size: {}\\n  File Name: {}\\n  The words shorter than {} are ignored by model\\n'.format(tabu_size, path, ignore))\n",
    "    file = open(path,'w')\n",
    "    for word in selected_spam_key.head(tabu_size).keyword:\n",
    "        file.write(word+'\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "32ea7bd3-f5c1-c7f5-19b8-677ec5dd6209"
   },
   "source": [
    "## 4. Read Tabu List and Convert SMS\n",
    "-----------------------------------\n",
    "Since the message is of variant length, it is not easy for the implementation of learning algorithm. So we define a Function above generating tabu list and storing them in the local file. And we can use this file to convert a SMS expressed in string to a vector of fixed length expressed in binary value.\n",
    "\n",
    "The idea is given like this: If we have a tabu list then we could find those word in the list and represent them by a index. Thus a string can be converted to an array of int. Further, we could define an array filled with zeros with the same length of tabu list. if this str contains the word in the tabu list, we could assign 1 to the corresponding element of the array representing 'message contains word w'. (tips: the query of `python.dict` is of constant time, much faster than `python.list`)\n",
    "\n",
    "By taking this step, we could convert our raw data of variant length into the numeric data of fixed length.\n",
    "\n",
    "These two function is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "46534282-54e9-4786-aabb-45df63148b66"
   },
   "outputs": [],
   "source": [
    "def read_tabu_list():\n",
    "    file = open('tabu.txt','r')\n",
    "    keyword_dict = dict()\n",
    "    i = 0\n",
    "    for line in file:\n",
    "        keyword_dict.update({line.strip():i})\n",
    "        i+=1\n",
    "    return keyword_dict\n",
    "\n",
    "def convert_Content(content, tabu):\n",
    "    m = len(tabu)\n",
    "    res = np.int_(np.zeros(m))\n",
    "    finds = re.findall('[A-Za-z]+', content)\n",
    "    for find in finds:\n",
    "        find=find.lower()\n",
    "        try:\n",
    "            i = tabu[find]\n",
    "            res[i]=1\n",
    "        except:\n",
    "            continue\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3a6fb48a-b327-cc67-bfe7-ee8dded0202b"
   },
   "source": [
    "## 5. Learning, Testing and Predicting\n",
    "-----------------------------------\n",
    "After we generate our tabu list and those supporting functions, we are now well prepared for the learning part in this problem. And here we could use the library `from sklearn.naive_bayes import BernoulliNB`. It will help us train this model.\n",
    "\n",
    "Before this part, let review our data: our feature input X is a n*m matrix, where X[i,j] = 1 means the sample #i contains the word j in the tabu list, and supervised label Y is a n*1 vector where Y[i] = 1 representing for a spam and 0 for a ham.\n",
    "\n",
    "Let prepare the materials for the learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "35051cb6-93f5-cc38-bab3-c94b1fb5ed91"
   },
   "outputs": [],
   "source": [
    "def learn():\n",
    "    global tabu, m\n",
    "    train,_ = readData()\n",
    "    n = train.shape[0]\n",
    "    X = np.zeros((n,m)); Y=np.int_(train.label=='spam')\n",
    "    for i in range(n):\n",
    "        X[i,:] = convert_Content(train.iloc[i].content, tabu)\n",
    "\n",
    "    NaiveBayes = BernoulliNB()\n",
    "    NaiveBayes.fit(X, Y)\n",
    "\n",
    "    Y_hat = NaiveBayes.predict(X)\n",
    "    print('>>>Learning...\\n  Learning Sample Size: {}\\n  Accuarcy (Training sample): {:.2f}％\\n'.format(n,sum(np.int_(Y_hat==Y))*100./n))\n",
    "    return NaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4f8040c8-7cc3-4db9-000e-58ae145ce898"
   },
   "source": [
    "The Function above returns a well trained Naive Bayes Model object, and we could use it to make prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "c98127ed-e089-9edd-eae0-97828efaad31"
   },
   "outputs": [],
   "source": [
    "def test(NaiveBayes):\n",
    "    global tabu, m\n",
    "    _,test = readData()\n",
    "    n = test.shape[0]\n",
    "    X = np.zeros((n,m)); Y=np.int_(test.label=='spam')\n",
    "    for i in range(n):\n",
    "        X[i,:] = convert_Content(test.iloc[i].content, tabu)\n",
    "    Y_hat = NaiveBayes.predict(X)\n",
    "    print ('>>>Cross Validation...\\n  Testing Sample Size: {}\\n  Accuarcy (Testing sample): {:.2f}％\\n'.format(n,sum(np.int_(Y_hat==Y))*100./n))\n",
    "    return\n",
    "\n",
    "def predictSMS(SMS):\n",
    "    global NaiveBayes, tabu, m\n",
    "    X = convert_Content(SMS, tabu)\n",
    "    Y_hat = NaiveBayes.predict(X.reshape(1,-1))\n",
    "    if int(Y_hat) == 1:\n",
    "        print ('SPAM: {}'.format(SMS))\n",
    "    else:\n",
    "        print ('HAM: {}'.format(SMS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "52bc3c19-af79-ece6-0379-a74c4b064e41"
   },
   "source": [
    "## 6. Overall Assembly\n",
    "-------------------\n",
    "After we define the every modules we need in this problem, we could integrate them into a whole part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "f4b028ff-859f-0977-2bc0-804ee5364504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCI SMS SPAM CLASSIFICATION PROBLEM SET\n",
      "  -- implemented by Bernoulli Naive Bayes Model\n",
      "\n",
      ">>>Generating Tabu List...\n",
      "  Tabu List Size: 300\n",
      "  File Name: tabu.txt\n",
      "  The words shorter than 3 are ignored by model\n",
      "\n",
      ">>>Learning...\n",
      "  Learning Sample Size: 4179\n",
      "  Accuarcy (Training sample): 98.28％\n",
      "\n",
      ">>>Cross Validation...\n",
      "  Testing Sample Size: 1393\n",
      "  Accuarcy (Testing sample): 97.92％\n",
      "\n",
      ">>>Testing\n",
      "HAM: Ya very nice. . .be ready on thursday\n",
      "SPAM: Had your mobile 10 mths? Update to the latest Camera/Video phones for FREE. KEEP UR SAME NUMBER, Get extra free mins/texts. Text YES for a call\n"
     ]
    }
   ],
   "source": [
    "print('UCI SMS SPAM CLASSIFICATION PROBLEM SET\\n  -- implemented by Bernoulli Naive Bayes Model\\n')\n",
    "tabu_file = 'tabu.txt'          # user defined tabu file\n",
    "tabu_size = 300                 # how many features are used to classify spam\n",
    "word_len_ignored = 3            # ignore those words shorter than this variable\n",
    "# build a tabu list based on the training data\n",
    "generate_tabu_list(tabu_file,tabu_size, word_len_ignored)\n",
    "\n",
    "tabu = read_tabu_list()\n",
    "m = len(tabu)\n",
    "# train the Naive Bayes Model using training data\n",
    "NaiveBayes=learn()\n",
    "# Test Model using testing data\n",
    "test(NaiveBayes)\n",
    "print('>>>Testing')\n",
    "# I select two messages from the test data here.\n",
    "predictSMS('Ya very nice. . .be ready on thursday')\n",
    "predictSMS('Had your mobile 10 mths? Update to the latest Camera/Video phones for FREE. KEEP UR SAME NUMBER, Get extra free mins/texts. Text YES for a call')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd0f767c-6ad4-f9bb-42f3-8a06f14f1f70"
   },
   "source": [
    "Okay, it works! A accuracy of 98.28% in training set and 97.77% is acceptable for me.\n",
    "\n",
    "Pleas feel free to ask me, if you have any question. And if you like this guide, please upvote, Thanks a lot.\n",
    "\n",
    "--Clyde Wang"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 2,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
